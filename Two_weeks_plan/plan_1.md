# Plan_1

1. ### 安装TensorFlow

   ##### （应用于各类机器学习算法的编程实现），python3（主要编程语言）

   #####      jupyter notebook（一款开放源代码的 Web 应用程序，可让我们创建并共享代码和文档），anaconda（一个开源的Python发行版本

   [TensorFlow的安装](https://www.cnblogs.com/lvsling/p/8672404.html)

   (因为很早前已经安装了,所以这里你们自己想办法安装,安装cpu版本的先.)

2. ### 学习机器学习的一些基础知识

   ​            (可以跳过,直接看视频入手)

   > + ####  使用神经网络识别手写数字
   >
   >   1. **感知器**
   >
   > ![1557994506047](./images/选区_001.png)
   >
   > 上面的这个东西就叫做感知器,
   >
   > x1,x2,x3,这些是二进制输入,他们都有属于他们的**权重** ,w1,w2,,,,,表示相应输入对于输出重要性的实数.
   >
   > output是神经元的输出,结果为0或1.由分配权重后的总和$$\sum{j}\omega_jx_j$$小于或者大于一些**阈值(threshold)** 决定
   >
   > ![1557996868665](./images/选区_002.png)
   >
   > 感知器网络:
   >
   > ![1557997234112](./images/选区_003.png)
   >
   > 上面有三层感知器,下一层的感知器都在权衡上一层的决策结果并做出决定,所以下一层的感知器可以比上一层中的做出更复杂和抽象的决策.以这种方式,一个多层的感知器网络可以从事复杂巧妙的决策
   >
   > 这里做两个变动
   >
   > ​				1.	把 $$ \sum{j}\omega_jx_j $$ 改写成**点乘** , $$ \omega\cdot x \equiv  \sum{j}\omega_jx_j $$  
   >
   > ​				2.	**偏置 b**  $$b \equiv -threshold$$ 
   >
   > 则感知器的规则可以重写为:
   >
   > ![1557997975881](./images/选区_004.png)
   >
   > (从此以后都不会出现阈值threshold这个东西了)
   >
   >   2. **S型神经元**
   >
   >      ![](./images/选区_001.png)
   >
   >      正如一个感知器,S 型神经元有多个输入,x 1 , x 2 , . . .。但是这些输入可以取 0 和 1 中的任意
   >      值,而不仅仅是 0 或 1。例如,0.638 . . . 是一个 S 型神经元的有效输入。同样,S 型神经元对每个
   >      输入有权重,w 1 , w 2 , . . .,和一个总的偏置,b。但是输出不是 0 或 1。相反,它现在是 σ(w · x + b),
   >      这里 σ 被称为 S 型函数 1 ,定义为:
   >
   >      ![选区_006](./images/选区_006.png)
   >
   >      把它们放在一起来更清楚地说明,一个具有输入 x 1 , x 2 , . . .,权重 w 1 , w 2 , . . .,和偏置 b 的 S
   >      型神经元的输出是:
   >
   >      ![选区_007](./images/选区_007.png)
   >
   >      σ 的代数形式又是什么?我们怎样去理解它呢?实际上,σ 的精确形式不重要 —— 重要的是
   >      这个函数绘制的形状。是这样:
   >
   >      ![选区_008](./images/选区_008.png)
   >
   >      
   >
   >      这个形状是阶跃函数平滑后的版本:
   >
   >      ![选区_010](./images/选区_010.png)
   >
   >      如果 σ 实际是个阶跃函数,既然输出会依赖于 w · x + b 是正数还是负数 2 ,那么 S 型神经元
   >      会成为一个感知器。利用实际的 σ 函数,我们得到一个,就像上面说明的,平滑的感知器。的
   >      确,σ 函数的平滑特性,正是关键因素,而不是其细部形式。σ 的平滑意味着权重和偏置的微小
   >      变化,即 ∆w j 和 ∆b,会从神经元产生一个微小的输出变化 ∆output。实际上,微积分告诉我们
   >      ∆output 可以很好地近似表示为:
   >
   >      ![选区_011](./images/选区_011.png)
   >
   >   3. **神经网络的框架**
   >
   >      ![选区_012](./images/选区_012.png)
   >
   >      别名:多层感知器/MLP
   >
   >   4. **一个简单的分类手写数字的网络**
   >
   >      ![选区_014](./images/选区_014.png)
   >
   >   5. **使用梯度下降算法**
   >
   >   6. **实现我们的网络来分类数字**
   >
   >      [MNIST入门]()
   >
   >      [MNIST改进]()
   >
   >   7. **迈向深度学习**
   >
   > + #### 反向传播算法如何工作
   >
   > + #### 改进神经网络的学习方法
   >
   > + #### 神经网络可以计算任何函数的可视化证明
   >
   > + #### 深度神经网络为何很难训练
   >
   > + #### 深度学习

   + #### 相关连接:

     > [书籍:	神经网络与深度学习](https://github.com/blime4/Mybook/blob/master/tensorflow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0neural%20networks%20and%20deep-learning-%E4%B8%AD%E6%96%87_ALL.pdf)
     >
     > [视频:	bilibili莫烦教程](https://www.bilibili.com/video/av16001891?from=search&seid=5385868079792534716)
     >
     > [视频:	bilibili另一个教程](https://www.bilibili.com/video/av35974848?from=search&seid=10055890662041855685)

3. ### 通过学习卷积神经网络CNN来初步了解tensorflow

4. ### 创建一个github项目共享我们的工作内容以及存放一些书籍文档

 

